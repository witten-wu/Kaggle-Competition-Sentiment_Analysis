{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Yidu WU\n",
    "\n",
    "**EID:** yiduwu2\n",
    "\n",
    "**Kaggle Team Name:** Yidu WU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 1 - Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:41.458543Z",
     "start_time": "2021-10-05T16:53:41.434940Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:42.276814Z",
     "start_time": "2021-10-05T16:53:42.268195Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "    txtdata = []\n",
    "    classes = []\n",
    "    topics  = []\n",
    "    with open(fname, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # get the text\n",
    "            txtdata.append(row[0])\n",
    "            # get the class (convert to integer)\n",
    "            if len(row)>1:\n",
    "                classes.append(row[1])\n",
    "                topics.append(row[2])\n",
    "    \n",
    "    if (len(classes)>0) and (len(txtdata) != len(classes)):        \n",
    "        raise Exception(\"mismatched length!\")\n",
    "    \n",
    "    return (txtdata, classes, topics)\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:44.351612Z",
     "start_time": "2021-10-05T16:53:44.335673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396\n",
      "1028\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "# (if using Kaggle notebooks you need to include the directory path: /kaggle/input/cs5489-2020b-assignment-1/)\n",
    "(traintxt, trainY, traintopic) = read_text_data(\"sanders_tweets_train.txt\")\n",
    "(testtxt, _, _)                = read_text_data(\"sanders_tweets_test.txt\")\n",
    "\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:45.842667Z",
     "start_time": "2021-10-05T16:53:45.493755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://t.co/QV4m1Un9 Forget the phone.. Nice UI. Liking the Scroll Feature #android #google #nexus', 'I hate when my phne do what it want on #twitter', '…and only the first 23 images in my Photo Roll made it over? Seriously, @apple, how did you fuck this up so much?', 'The lock screen now has facial recognition capability! #Google #Android #ICS', '#TeamGoogleNexus RT @B__Y #Google + #Samsung = Perfect #Icecream sandwich #GalaxyNexus', \"RT @FierceWireless: #Microsoft's Ballmer promises #Nokia Windows phones next week, slams #Android.  http://t.co/OH8Btu2e #WP7 #Window...\", '@Slater_Boy Needs to take his ass to sleep... Its grown folks hour on #twitter', 'RT @bmann Awesome! @TommyLee is moving to #Vancouver as Dev Evangelist for #Microsoft - we finally get a local', 'Have never had such poor customer service at @Apple before! What happened? (@ Apple Store w/ 2 others) http://t.co/GKlXMUi6', 'Thank you @apple for your innovations. Exhibit A: Guy playing with Facetime instead of watching game at sports bar. http://t.co/oU7K39ge', 'I cannot wait for a new #ICS phone!!! #Google #android', 'A Child’s-Eye View Of #Microsoft’s #Kinect For #Kids http://t.co/mnamu0xF', '37 Ways to Use #Twitter for Business\\thttp://t.co/HGhMpoSE', '#Android #Google Google updates Nexus site with Galaxy Nexus details http://t.co/BeckhmA1 #DhilipSiva', 'Thank you #twitter for not giving me the 10 tweets that have been tweeted at me in the past 3 days! Lol #blameitontheiphone #ineedtoupdate', 'Funny #BillGates #Aprons w/Google #1 ranked #LTCartoon “Got #Microsoft“? http://t.co/CStd8cPM #humor #foodie #microsoft', 'iPhone 4S 8MP camera : It´s a SONY. http://t.co/njTHOi2x @sony @apple #apple #iphone4S #cameras', 'Yahoo Profit Drops 26 Per Cent! http://t.co/7AdG1Hss #yahoo #microsoft', 'Soon. Getting ready for the party. #google #android #icecreamsandwich http://t.co/eZjCzLt6', \"I need 2 unfollow some of these wannabe G's! RT @SteveGr8ofTeeZ: RT @The_Kase Do real thugs have #twitter? #confusedbymytimeline  //Hell no.\"]\n",
      "\n",
      "process step1\n",
      "[' Forget the phone Nice UI Liking the Scroll Feature ', 'I hate when my phne do what it want on ', ' and only the first  images in my Photo Roll made it over Seriously how did you fuck this up so much ', 'The lock screen now has facial recognition capability ', ' RT Perfect sandwich ', 'RT s Ballmer promises Windows phones next week slams ', ' Needs to take his ass to sleep Its grown folks hour on ', 'RT Awesome is moving to as Dev Evangelist for we finally get a local', 'Have never had such poor customer service at before What happened Apple Store w  others ', 'Thank you for your innovations Exhibit A Guy playing with Facetime instead of watching game at sports bar ', 'I cannot wait for a new phone ', 'A Child s Eye View Of s For ', ' Ways to Use for Business ', ' Google updates Nexus site with Galaxy Nexus details ', 'Thank you for not giving me the  tweets that have been tweeted at me in the past  days Lol ', 'Funny w Google ranked Got ', 'iPhone S MP camera It s a SONY ', 'Yahoo Profit Drops  Per Cent ', 'Soon Getting ready for the party ', 'I need  unfollow some of these wannabe G s RT RT Do real thugs have Hell no ']\n",
      "\n",
      "process step2\n",
      "['Forget phone Nice Liking Scroll Feature', 'hate when phne what want', 'only first images Photo Roll made over Seriously fuck this much', 'lock screen facial recognition capability', 'Perfect sandwich', 'Ballmer promises Windows phones next week slams', 'Needs take sleep grown folks hour', 'Awesome moving Evangelist finally local', 'Have never such poor customer service before What happened Apple Store others', 'Thank your innovations Exhibit playing with Facetime instead watching game sports', 'cannot wait phone', 'Child View', 'Ways Business', 'Google updates Nexus site with Galaxy Nexus details', 'Thank giving tweets that have been tweeted past days', 'Funny Google ranked', 'iPhone camera SONY', 'Yahoo Profit Drops Cent', 'Soon Getting ready party', 'need unfollow some these wannabe real thugs have Hell']\n"
     ]
    }
   ],
   "source": [
    "# Deal with train data\n",
    "# Delete unrelated words by the Regular expression\n",
    "import re\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)    \n",
    "    return input_txt  \n",
    "\n",
    "def replace_all_blank(value):\n",
    "    result = re.sub('\\W+', ' ', value).replace(\"_\", '')\n",
    "    return result\n",
    "\n",
    "#delete some special words\n",
    "print(traintxt[0:20])\n",
    "print(\"\")\n",
    "print(\"process step1\")\n",
    "traintxtTemp = []\n",
    "for i in range(len(traintxt)):\n",
    "    t = remove_pattern(traintxt[i], \"#[\\w]*\")            #Delete #\n",
    "    t = remove_pattern(t, \"@[\\w]*\")                      #Delete @ \n",
    "    t = remove_pattern(t, r'http://[a-zA-Z0-9.?/&=:]*')  #Delete Url\n",
    "    t = replace_all_blank(t)                             \n",
    "    t = remove_pattern(t, r\"[0-9]+\")                     #Delete number\n",
    "    traintxtTemp.append(t)\n",
    "print(traintxtTemp[0:20])\n",
    "print(\"\")\n",
    "print(\"process step2\")\n",
    "\n",
    "#Delete words which length less than 4\n",
    "traintxtTemp2 = []\n",
    "f = lambda x: ' '.join([w for w in x.split() if len(w)>3])\n",
    "traintxtTemp2 = list(map(f, traintxtTemp))\n",
    "print(traintxtTemp2[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T09:37:58.897558Z",
     "start_time": "2021-10-05T09:37:58.746026Z"
    }
   },
   "outputs": [],
   "source": [
    "# install nltk to ues the Stemming function\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:49.250101Z",
     "start_time": "2021-10-05T16:53:48.823221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Forget phone Nice Liking Scroll Feature', 'hate when phne what want', 'only first images Photo Roll made over Seriously fuck this much', 'lock screen facial recognition capability', 'Perfect sandwich', 'Ballmer promises Windows phones next week slams', 'Needs take sleep grown folks hour', 'Awesome moving Evangelist finally local', 'Have never such poor customer service before What happened Apple Store others', 'Thank your innovations Exhibit playing with Facetime instead watching game sports', 'cannot wait phone', 'Child View', 'Ways Business', 'Google updates Nexus site with Galaxy Nexus details', 'Thank giving tweets that have been tweeted past days', 'Funny Google ranked', 'iPhone camera SONY', 'Yahoo Profit Drops Cent', 'Soon Getting ready party', 'need unfollow some these wannabe real thugs have Hell']\n",
      "\n",
      "process step3\n",
      "[['Forget', 'phone', 'Nice', 'Liking', 'Scroll', 'Feature'], ['hate', 'when', 'phne', 'what', 'want'], ['only', 'first', 'images', 'Photo', 'Roll', 'made', 'over', 'Seriously', 'fuck', 'this', 'much'], ['lock', 'screen', 'facial', 'recognition', 'capability'], ['Perfect', 'sandwich'], ['Ballmer', 'promises', 'Windows', 'phones', 'next', 'week', 'slams'], ['Needs', 'take', 'sleep', 'grown', 'folks', 'hour'], ['Awesome', 'moving', 'Evangelist', 'finally', 'local'], ['Have', 'never', 'such', 'poor', 'customer', 'service', 'before', 'What', 'happened', 'Apple', 'Store', 'others'], ['Thank', 'your', 'innovations', 'Exhibit', 'playing', 'with', 'Facetime', 'instead', 'watching', 'game', 'sports'], ['cannot', 'wait', 'phone'], ['Child', 'View'], ['Ways', 'Business'], ['Google', 'updates', 'Nexus', 'site', 'with', 'Galaxy', 'Nexus', 'details'], ['Thank', 'giving', 'tweets', 'that', 'have', 'been', 'tweeted', 'past', 'days'], ['Funny', 'Google', 'ranked'], ['iPhone', 'camera', 'SONY'], ['Yahoo', 'Profit', 'Drops', 'Cent'], ['Soon', 'Getting', 'ready', 'party'], ['need', 'unfollow', 'some', 'these', 'wannabe', 'real', 'thugs', 'have', 'Hell']]\n",
      "\n",
      "process step4\n",
      "[['forget', 'phone', 'nice', 'like', 'scroll', 'featur'], ['hate', 'when', 'phne', 'what', 'want'], ['onli', 'first', 'imag', 'photo', 'roll', 'made', 'over', 'serious', 'fuck', 'thi', 'much'], ['lock', 'screen', 'facial', 'recognit', 'capabl'], ['perfect', 'sandwich'], ['ballmer', 'promis', 'window', 'phone', 'next', 'week', 'slam'], ['need', 'take', 'sleep', 'grown', 'folk', 'hour'], ['awesom', 'move', 'evangelist', 'final', 'local'], ['have', 'never', 'such', 'poor', 'custom', 'servic', 'befor', 'what', 'happen', 'appl', 'store', 'other'], ['thank', 'your', 'innov', 'exhibit', 'play', 'with', 'facetim', 'instead', 'watch', 'game', 'sport'], ['cannot', 'wait', 'phone'], ['child', 'view'], ['way', 'busi'], ['googl', 'updat', 'nexu', 'site', 'with', 'galaxi', 'nexu', 'detail'], ['thank', 'give', 'tweet', 'that', 'have', 'been', 'tweet', 'past', 'day'], ['funni', 'googl', 'rank'], ['iphon', 'camera', 'soni'], ['yahoo', 'profit', 'drop', 'cent'], ['soon', 'get', 'readi', 'parti'], ['need', 'unfollow', 'some', 'these', 'wannab', 'real', 'thug', 'have', 'hell']]\n"
     ]
    }
   ],
   "source": [
    "# Deal with train data\n",
    "# use Stemming to extract the stem\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(traintxtTemp2[0:20])\n",
    "print(\"\")\n",
    "print(\"process step3\")\n",
    "traintxtTemp3 = []\n",
    "f = lambda x: x.split()\n",
    "traintxtTemp3 = list(map(f, traintxtTemp2))\n",
    "print(traintxtTemp3[0:20])\n",
    "print(\"\")\n",
    "\n",
    "print(\"process step4\")\n",
    "traintxtNew = []\n",
    "f=lambda x: [porter_stemmer.stem(i) for i in x]\n",
    "traintxtNew = list(map(f, traintxtTemp3))\n",
    "print(traintxtNew[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T09:38:12.503232Z",
     "start_time": "2021-10-05T09:38:12.479348Z"
    }
   },
   "outputs": [],
   "source": [
    "# install mosestokenizer to combine the words which have been stemmed\n",
    "pip install mosestokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:53:53.302588Z",
     "start_time": "2021-10-05T16:53:52.680172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process step5\n",
      "['forget phone nice like scroll featur', 'hate when phne what want', 'onli first imag photo roll made over serious fuck thi much', 'lock screen facial recognit capabl', 'perfect sandwich', 'ballmer promis window phone next week slam', 'need take sleep grown folk hour', 'awesom move evangelist final local', 'have never such poor custom servic befor what happen appl store other', 'thank your innov exhibit play with facetim instead watch game sport', 'cannot wait phone', 'child view', 'way busi', 'googl updat nexu site with galaxi nexu detail', 'thank give tweet that have been tweet past day', 'funni googl rank', 'iphon camera soni', 'yahoo profit drop cent', 'soon get readi parti', 'need unfollow some these wannab real thug have hell']\n"
     ]
    }
   ],
   "source": [
    "# Deal with train data\n",
    "# use mosestokenizer to combine the words\n",
    "from mosestokenizer import MosesDetokenizer\n",
    "detokenizer = MosesDetokenizer()\n",
    "for i in range(len(traintxtNew)):\n",
    "    traintxtNew[i] = detokenizer(traintxtNew[i])\n",
    "    \n",
    "print(\"process step5\")\n",
    "print(traintxtNew[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T16:54:00.271928Z",
     "start_time": "2021-10-05T16:53:59.351760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What do we have to do to get Flash on #iPad? @Apple please please please change your mind. Thank you in advance! @kusi_sk  @PeterNemcok', 'RT @RUILIFESTYLE: #GOOGLE THE MIXTAPE http://t.co/v8zTtNVV', 'Apples to oranges maybe, but #Twitter is way more engaging than #FB. Content vs. Stalkerismo', 'Thanks @apple for an OS update that has only succesfully bricked my iPod.', 'WHTS GOIN ON #TWITTER FAM', 'Awesome service from the @apple store in pc. Thanks chris!', 'All this talk about about how good #icecreamsandwich looks is making me hungry #google #omnomnomnom', \"RT @EcommNewsUpdate Steve Yegge's Google Plus Rant - Google's Pathetic Afterthought http://t.co/4Tt5OsvN #blog #facebook #google #googleplus\", \"Comp crash #Microsoft :/ Wouldn't get this with an iMac -__-\", '@Affan Fact! They still do everything better though #google', '#Microsoft Research shows #Holodesk - looks like science fiction, but is science fact http://t.co/vv6ltkYp', 'Microsoft applies for celebrity search patent #microsoft #bing: Microsoft has applied for a new patent that take... http://t.co/4qoQqDCY', '► #Job #IT #Projekt #Management #Microsoft  | Projekt für Freelancer: Programm Manager (m/w) in Berlin » http://t.co/MO4jtpTL', \"@apple - you have invented a product that actually gets my brother to call my parents when he gets where he's going. Amazing. #siri #ipod4s\", '@DailyDealChat @apple Thanks!!', \"first thing on #google's to do list: hire a performance coach.\", '#twitter off :) good night for everybody !', \"My thoughts on tonight's #Google Ice Cream Sandwich and #Samsung Galaxy Nexus talk: http://t.co/gu7ScKXL\", '@trisha_ps @iCloud is Cloudy. @apple tech, not very techy.', '#Google #Android Icecream Sandwich OS for Nexus Prime Released! [LIVE BLOG] | http://t.co/LF03ZKUz #technology #hk']\n",
      "\n",
      "process step1\n",
      "['What do we have to do to get Flash on please please please change your mind Thank you in advance ', 'RT THE MIXTAPE ', 'Apples to oranges maybe but is way more engaging than Content vs Stalkerismo', 'Thanks for an OS update that has only succesfully bricked my iPod ', 'WHTS GOIN ON FAM', 'Awesome service from the store in pc Thanks chris ', 'All this talk about about how good looks is making me hungry ', 'RT Steve Yegge s Google Plus Rant Google s Pathetic Afterthought plus', 'Comp crash Wouldn t get this with an iMac  ', ' Fact They still do everything better though ', ' Research shows looks like science fiction but is science fact ', 'Microsoft applies for celebrity search patent Microsoft has applied for a new patent that take ', ' Projekt für Freelancer Programm Manager m w in Berlin ', ' you have invented a product that actually gets my brother to call my parents when he gets where he s going Amazing ', ' Thanks ', 'first thing on s to do list hire a performance coach ', ' off good night for everybody ', 'My thoughts on tonight s Ice Cream Sandwich and Galaxy Nexus talk ', ' is Cloudy tech not very techy ', ' Icecream Sandwich OS for Nexus Prime Released LIVE BLOG ']\n",
      "\n",
      "process step2\n",
      "['What have Flash please please please change your mind Thank advance', 'MIXTAPE', 'Apples oranges maybe more engaging than Content Stalkerismo', 'Thanks update that only succesfully bricked iPod', 'WHTS GOIN', 'Awesome service from store Thanks chris', 'this talk about about good looks making hungry', 'Steve Yegge Google Plus Rant Google Pathetic Afterthought plus', 'Comp crash Wouldn this with iMac', 'Fact They still everything better though', 'Research shows looks like science fiction science fact', 'Microsoft applies celebrity search patent Microsoft applied patent that take', 'Projekt Freelancer Programm Manager Berlin', 'have invented product that actually gets brother call parents when gets where going Amazing', 'Thanks', 'first thing list hire performance coach', 'good night everybody', 'thoughts tonight Cream Sandwich Galaxy Nexus talk', 'Cloudy tech very techy', 'Icecream Sandwich Nexus Prime Released LIVE BLOG']\n",
      "\n",
      "process step3\n",
      "[['What', 'have', 'Flash', 'please', 'please', 'please', 'change', 'your', 'mind', 'Thank', 'advance'], ['MIXTAPE'], ['Apples', 'oranges', 'maybe', 'more', 'engaging', 'than', 'Content', 'Stalkerismo'], ['Thanks', 'update', 'that', 'only', 'succesfully', 'bricked', 'iPod'], ['WHTS', 'GOIN'], ['Awesome', 'service', 'from', 'store', 'Thanks', 'chris'], ['this', 'talk', 'about', 'about', 'good', 'looks', 'making', 'hungry'], ['Steve', 'Yegge', 'Google', 'Plus', 'Rant', 'Google', 'Pathetic', 'Afterthought', 'plus'], ['Comp', 'crash', 'Wouldn', 'this', 'with', 'iMac'], ['Fact', 'They', 'still', 'everything', 'better', 'though'], ['Research', 'shows', 'looks', 'like', 'science', 'fiction', 'science', 'fact'], ['Microsoft', 'applies', 'celebrity', 'search', 'patent', 'Microsoft', 'applied', 'patent', 'that', 'take'], ['Projekt', 'Freelancer', 'Programm', 'Manager', 'Berlin'], ['have', 'invented', 'product', 'that', 'actually', 'gets', 'brother', 'call', 'parents', 'when', 'gets', 'where', 'going', 'Amazing'], ['Thanks'], ['first', 'thing', 'list', 'hire', 'performance', 'coach'], ['good', 'night', 'everybody'], ['thoughts', 'tonight', 'Cream', 'Sandwich', 'Galaxy', 'Nexus', 'talk'], ['Cloudy', 'tech', 'very', 'techy'], ['Icecream', 'Sandwich', 'Nexus', 'Prime', 'Released', 'LIVE', 'BLOG']]\n",
      "\n",
      "process step4\n",
      "[['what', 'have', 'flash', 'pleas', 'pleas', 'pleas', 'chang', 'your', 'mind', 'thank', 'advanc'], ['mixtap'], ['appl', 'orang', 'mayb', 'more', 'engag', 'than', 'content', 'stalkerismo'], ['thank', 'updat', 'that', 'onli', 'succes', 'brick', 'ipod'], ['wht', 'goin'], ['awesom', 'servic', 'from', 'store', 'thank', 'chri'], ['thi', 'talk', 'about', 'about', 'good', 'look', 'make', 'hungri'], ['steve', 'yegg', 'googl', 'plu', 'rant', 'googl', 'pathet', 'afterthought', 'plu'], ['comp', 'crash', 'wouldn', 'thi', 'with', 'imac'], ['fact', 'they', 'still', 'everyth', 'better', 'though'], ['research', 'show', 'look', 'like', 'scienc', 'fiction', 'scienc', 'fact'], ['microsoft', 'appli', 'celebr', 'search', 'patent', 'microsoft', 'appli', 'patent', 'that', 'take'], ['projekt', 'freelanc', 'programm', 'manag', 'berlin'], ['have', 'invent', 'product', 'that', 'actual', 'get', 'brother', 'call', 'parent', 'when', 'get', 'where', 'go', 'amaz'], ['thank'], ['first', 'thing', 'list', 'hire', 'perform', 'coach'], ['good', 'night', 'everybodi'], ['thought', 'tonight', 'cream', 'sandwich', 'galaxi', 'nexu', 'talk'], ['cloudi', 'tech', 'veri', 'techi'], ['icecream', 'sandwich', 'nexu', 'prime', 'releas', 'live', 'blog']]\n",
      "\n",
      "process step5\n",
      "['what have flash pleas pleas pleas chang your mind thank advanc', 'mixtap', 'appl orang mayb more engag than content stalkerismo', 'thank updat that onli succes brick ipod', 'wht goin', 'awesom servic from store thank chri', 'thi talk about about good look make hungri', 'steve yegg googl plu rant googl pathet afterthought plu', 'comp crash wouldn thi with imac', 'fact they still everyth better though', 'research show look like scienc fiction scienc fact', 'microsoft appli celebr search patent microsoft appli patent that take', 'projekt freelanc programm manag berlin', 'have invent product that actual get brother call parent when get where go amaz', 'thank', 'first thing list hire perform coach', 'good night everybodi', 'thought tonight cream sandwich galaxi nexu talk', 'cloudi tech veri techi', 'icecream sandwich nexu prime releas live blog']\n"
     ]
    }
   ],
   "source": [
    "# Deal with test data\n",
    "# Delete unrelated words by the Regular expression\n",
    "print(testtxt[0:20])\n",
    "print(\"\")\n",
    "print(\"process step1\")\n",
    "testtxtTemp = []\n",
    "for i in range(len(testtxt)):\n",
    "    t = remove_pattern(testtxt[i], \"#[\\w]*\")            #Delete #\n",
    "    t = remove_pattern(t, \"@[\\w]*\")                      #Delete @ \n",
    "    t = remove_pattern(t, r'http://[a-zA-Z0-9.?/&=:]*')  #Delete Url\n",
    "    t = replace_all_blank(t)                             \n",
    "    t = remove_pattern(t, r\"[0-9]+\")                     #Delete number\n",
    "    testtxtTemp.append(t)\n",
    "print(testtxtTemp[0:20])\n",
    "print(\"\")\n",
    "print(\"process step2\")\n",
    "\n",
    "#Delete words which length less than 4\n",
    "testtxtTemp2 = []\n",
    "f = lambda x: ' '.join([w for w in x.split() if len(w)>3])\n",
    "testtxtTemp2 = list(map(f, testtxtTemp))\n",
    "print(testtxtTemp2[0:20])\n",
    "\n",
    "# use Stemming to extract the stem\n",
    "print(\"\")\n",
    "print(\"process step3\")\n",
    "testtxtTemp3 = []\n",
    "f = lambda x: x.split()\n",
    "testtxtTemp3 = list(map(f, testtxtTemp2))\n",
    "print(testtxtTemp3[0:20])\n",
    "print(\"\")\n",
    "\n",
    "print(\"process step4\")\n",
    "testtxtNew = []\n",
    "f=lambda x: [porter_stemmer.stem(i) for i in x]\n",
    "testtxtNew = list(map(f, testtxtTemp3))\n",
    "print(testtxtNew[0:20])\n",
    "\n",
    "# use mosestokenizer to combine the words\n",
    "detokenizer_test = MosesDetokenizer()\n",
    "for i in range(len(testtxtNew)):\n",
    "    testtxtNew[i] = detokenizer_test(testtxtNew[i])\n",
    "\n",
    "print(\"\")\n",
    "print(\"process step5\")\n",
    "print(testtxtNew[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T17:03:22.596253Z",
     "start_time": "2021-10-05T16:54:52.448601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 400 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 704 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1154 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2354 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3104 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'neutral' ... 'neutral' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# Use the SVM with RBF kernel\n",
    "cntvect = feature_extraction.text.CountVectorizer(stop_words='english')\n",
    "cntvect.fit(traintxtNew)\n",
    "trainX = cntvect.transform(traintxtNew)\n",
    "testX  = cntvect.transform(testtxtNew)\n",
    "paramgrid = {'C': logspace(-2,3,20), 'gamma': logspace(-4,3,20) }\n",
    "svmcv = model_selection.GridSearchCV(svm.SVC(kernel='rbf'), paramgrid, cv=10,\n",
    "                                    n_jobs=-1, verbose=True)\n",
    "svmcv.fit(trainX, trainY);\n",
    "predY = svmcv.predict(testX)\n",
    "print(predY)\n",
    "write_csv_kaggle_sub(\"my_submission.csv\", predY)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
